---
author: Ramblings of a supercomputing enthusiast.
author_tag: gaborsamu
blog_subtitle: Recent content in Blogs on Technical Computing Goulash
blog_title: Blogs on Technical Computing Goulash
blog_url: https://www.gaborsamu.com/blog/
category: gaborsamu
date: '2013-10-09 19:12:15'
layout: post
original_url: https://www.gaborsamu.com/blog/hpc411_bridge/
slug: ibm-platform-hpc-4-1-1-creating-a-network-bridge-on-compute-nodes
title: IBM Platform HPC 4.1.1- Creating a network bridge on compute nodes
---

<p><strong>Applies to</strong></p>

<ul>
<li>IBM Platform HPC V4.1.1</li>
<li>IBM Platform Cluster Manager V4.1.1</li>
</ul>
<p><strong>Introduction</strong></p>

<p>IBM Platform HPC provides the ability to customise the network configuration
of compute nodes via Network Profiles. Network Profiles support a custom NIC
script for each defined interface.</p>

<p>This provides the ability to configure network bonding and bridging. Here we
provide a detailed example on how to configure a network bridge in a cluster
managed by IBM Platform HPC.</p>

<p>IBM Platform HPC includes xCAT technology for cluster provisioning. xCAT
includes a script (<em>/install/postscripts/xHRM</em>) which may be used  to
configure network bridging. This script is leveraged as a custom network
script in the example below.</p>

<p><strong>Example</strong></p>

<p>The configuration of the network provision may be viewed in the IBM Platform HPC Web console at: <em>Resources &gt; Node Provisioning &gt; Networks</em>.</p>

<figure><img src="https://www.gaborsamu.com/images/provision_net_wiki.png" />
</figure>

<p>The configuration of network provision may also be viewed using the <em>lsdef</em> CLI.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># lsdef -t network provision</span>
Object name: provision
    domain<span style="color: #f92672;">=</span>private.dns.zone
    dynamicrange<span style="color: #f92672;">=</span>192.0.2.201-192.0.2.254
    gateway<span style="color: #f92672;">=</span>&lt;xcatmaster&gt;
    mask<span style="color: #f92672;">=</span>255.255.255.0
    mgtifname<span style="color: #f92672;">=</span>eth0
    net<span style="color: #f92672;">=</span>192.0.2.0
    staticrange<span style="color: #f92672;">=</span>192.0.2.15-192.0.2.49
    staticrangeincrement<span style="color: #f92672;">=</span><span style="color: #ae81ff;">1</span>
    tftpserver<span style="color: #f92672;">=</span>192.0.2.50</code></pre></div>

<p>The Network Profile <em>default_network_profile</em> which includes the network
provision may be viewed in the IBM Platform HPC Web console at: <em>Resources &gt;
Node Provisioning &gt; Provisioning Templates &gt; Network Profiles</em>.</p>

<p>{{ &lt; figure src=&quot;/images/network_profile_provision_wiki.png&quot; &gt;}}</p>

<p>The Network Profile <em>default_network_profile</em> configuration may also be viewed
using the <em>lsdef</em> CLI.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># lsdef -t group __NetworkProfile_default_network_profile</span>
Object name: __NetworkProfile_default_network_profile
    grouptype<span style="color: #f92672;">=</span>static
    installnic<span style="color: #f92672;">=</span>eth0
    members<span style="color: #f92672;">=</span>
    netboot<span style="color: #f92672;">=</span>xnba
    nichostnamesuffixes.eth0<span style="color: #f92672;">=</span>-eth0
    nichostnamesuffixes.bmc<span style="color: #f92672;">=</span>-bmc
    nicnetworks.eth0<span style="color: #f92672;">=</span>provision
    nicnetworks.bmc<span style="color: #f92672;">=</span>provision
    nictypes.eth0<span style="color: #f92672;">=</span>Ethernet
    nictypes.bmc<span style="color: #f92672;">=</span>BMC
    primarynic<span style="color: #f92672;">=</span>eth0</code></pre></div>

<p>Here, we configure a network bridge <em>br0</em> against <em>eth0</em> for compute nodes
using a new Network Profile.</p>

<ol>
<li>Add a new Network Profile with name <em>default_network_profile_bridge</em> via
the IBM Platform HPC Web console. As an Administrator user, browse to <em>Resources &gt; Node Provisioning &gt; Provisioning Templates &gt; Network Profiles</em> and select
the button <em>Add</em>.</li>
</ol>
<figure><img src="https://www.gaborsamu.com/images/new_profile_wiki.png" />
</figure>

<p>A total of three devices are required to be added:</p>

<ul>
<li>
<p>eth0</p>

</li>
<li>
<p>Type: Ethernet</p>

</li>
<li>
<p>Network: provision</p>

</li>
<li>
<p>bmc</p>

</li>
<li>
<p>Type: BMC</p>

</li>
<li>
<p>Network: provision</p>

</li>
<li>
<p>br0</p>

</li>
<li>
<p>Type: Customized</p>

</li>
<li>
<p>Network: provision</p>

</li>
<li>
<p>Configuration Command: xHRM bridgeprereq eth0:br0 (creates network bridge
br0 against eth0)</p>

</li>
</ul>
<p>The new Network Profile <em>default_network_profile_bridge</em> is shown below.</p>

<figure><img src="https://www.gaborsamu.com/images/new_profile5_wiki.png" />
</figure>

<ol start="2">
<li>Now we are ready to provision the nodes using the new Network Profile
<em>default_network_profile_bridge</em>. To begin the process to add nodes, navigate
in the the IBM Platform HPC Web console to <em>Resources &gt; Devices &gt; Nodes</em> and
select the button <em>Add</em>. Within the Add Nodes window, select optionally
Node Group <em>compute</em> and Select Specify Properties for the provisioning
template. This will allow you to select the newly created network profile
<em>default_network_profile_bridge</em>. Here the hardware profile <em>IPMI</em> and stateful
provisioning are used.</li>
</ol>
<figure><img src="https://www.gaborsamu.com/images/add_nodes_wiki.png" />
</figure>

<p>Nodes are added using Auto discovery by PXE boot.  Nodes may also be added
using a node information file.</p>

<p>The nodes are powered on, detected by IBM Platform HPC and provisioned.  In
this example, two nodes <em>compute000</em>, <em>compute001</em> are detected and
subsequently provisioned.</p>

<ol start="3">
<li>Once the nodes have been provisioned and complete their initial boot, they
appear in the IBM Platform HPC Web console (<em>Resources &gt; Devices &gt; Nodes</em>) with
Status <em>booted</em> and Workload Agent <em>OK</em>.</li>
</ol>
<figure><img src="https://www.gaborsamu.com/images/nodes_wiki.png" />
</figure>

<p>The network bridge is configured on the nodes as expected.  We may see this
via the IBM Platform HPC Web console by browsing to <em>Resources &gt; Devices &gt;
Nodes</em> and selecting the <em>Summary</em> tab and scrolling to <em>Other Key Properties</em>.</p>

<figure><img src="https://www.gaborsamu.com/images/nodes2_wiki.png" />
</figure>

<p>Finally, using the CLI <em>xdsh</em>, we remotely execute ifconfig on node <em>compute001</em>to check the configuration of interface <em>br0</em>.</p>

<div class="highlight"><pre><code class="language-bash"><span style="color: #75715e;"># xdsh compute001 ifconfig br0</span>
compute001: br0       Link encap:Ethernet  HWaddr 00:1E:67:49:CC:E5   
compute001:           inet addr:192.0.2.20  Bcast:192.0.2.255  Mask:255.255.255.0
compute001:           inet6 addr: fe80::b03b:7cff:fe61:c1d4/64 Scope:Link
compute001:           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
compute001:           RX packets:26273 errors:0 dropped:0 overruns:0 frame:0
compute001:           TX packets:42490 errors:0 dropped:0 overruns:0 carrier:0
compute001:           collisions:0 txqueuelen:0  
compute001:           RX bytes:11947435 <span style="color: #f92672;">(</span>11.3 MiB<span style="color: #f92672;">)</span>  TX bytes:7827365 <span style="color: #f92672;">(</span>7.4 MiB<span style="color: #f92672;">)</span>
compute001:</code></pre></div>

<p>As expected, the compute nodes have been provisioned with a network bridge
<em>br0</em> configured.</p>